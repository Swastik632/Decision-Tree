{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Q1) What is a Decision Tree, and how does it work in the context of\n",
        "classification?\n",
        "\n",
        "Ans1)\n",
        "\n",
        "    A Decision Tree is a supervised machine learning algorithm that is widely used for classification problems. It works by breaking down a dataset into smaller subsets based on feature values, eventually forming a tree-like structure of decisions. At each internal node, the algorithm selects the feature that best splits the data into groups that are as homogeneous as possible with respect to the target variable. Techniques like Information Gain, Entropy, or Gini Index are used to determine the “best” split. The branches represent the outcomes of a decision, while the leaf nodes represent the final classification labels.\n",
        "\n",
        "    For example, in a customer dataset, a decision tree might first split on “Age,” then on “Income,” and finally on “Purchase History” to decide whether a customer will buy a product or not. The step-by-step splitting continues until the data is divided into pure groups or until a stopping condition is reached. Decision trees are easy to interpret and visualize, making them helpful for understanding how classification decisions are made. However, they can sometimes overfit the data, which is why techniques like pruning or ensemble methods (Random Forests, Gradient Boosted Trees) are often used to improve performance.\n",
        "\n",
        "Q2) Explain the concepts of Gini Impurity and Entropy as impurity measures.\n",
        "How do they impact the splits in a Decision Tree?\n",
        "\n",
        "Ans2)\n",
        "\n",
        "    Gini Impurity and Entropy are two common impurity measures used in decision trees to decide the best feature for splitting the data.\n",
        "\n",
        "    Gini Impurity measures how often a randomly chosen element from the dataset would be incorrectly classified if it was randomly labeled according to the class distribution. A Gini value of 0 means the node is pure (all samples belong to one class), while higher values mean more impurity.\n",
        "\n",
        "    Entropy, from information theory, measures the level of uncertainty or disorder in the data. If all samples in a node belong to the same class, entropy is 0, but if classes are equally mixed, entropy is at its maximum.\n",
        "\n",
        "    During tree construction, the algorithm tries to split the data in a way that reduces impurity the most—this is called Information Gain when using entropy and Gini Gain when using Gini Impurity. In practice, both measures usually lead to similar trees, but Gini is computationally faster, while entropy gives more weight to less frequent classes. By minimizing impurity at each step, the decision tree becomes better at separating the classes and making accurate predictions.\n",
        "  \n",
        "Q3) What is the difference between Pre-Pruning and Post-Pruning in Decision\n",
        "Trees? Give one practical advantage of using each.\n",
        "\n",
        "Ans3)\n",
        "\n",
        "    Pre-pruning and Post-pruning are techniques used to prevent overfitting in decision trees.\n",
        "\n",
        "    Pre-pruning (Early Stopping): The tree growth is stopped early before it becomes too complex. Conditions like maximum depth, minimum samples per split, or minimum information gain are set to control the tree size.\n",
        "\n",
        "    Advantage: Saves computation time and prevents overly complex trees right from the start.\n",
        "\n",
        "    Post-pruning (Pruning after Full Growth): The tree is allowed to grow fully, and then unnecessary branches that do not improve accuracy are cut back. This is done using validation data or statistical measures.\n",
        "\n",
        "      Advantage: Produces a simpler tree that generalizes better while still considering all possible splits during training.\n",
        "\n",
        "Q4) What is Information Gain in Decision Trees, and why is it important for\n",
        "choosing the best split?\n",
        "\n",
        "Ans4)\n",
        "\n",
        "    Information Gain is a measure used in decision trees to determine which feature provides the best split at a node. It is based on the concept of entropy, which measures the level of disorder or uncertainty in a dataset. When the data is split on a particular feature, the entropy of the resulting subsets is compared with the entropy of the original dataset. Information Gain is the reduction in entropy after the split.\n",
        "\n",
        "    It is important because a higher information gain means the feature creates purer groups (more homogeneous with respect to the target class), which improves the decision tree’s ability to classify data accurately. By always choosing the feature with the highest information gain, the decision tree grows in a way that maximizes learning and minimizes uncertainty at each step.\n",
        "\n",
        "Q5) What are some common real-world applications of Decision Trees, and\n",
        "what are their main advantages and limitations?\n",
        "\n",
        "Ans5)\n",
        "\n",
        "    Decision trees are widely used in real-world applications because they are simple to understand and effective for both classification and regression tasks. Some common applications include:\n",
        "\n",
        "    Business & Marketing: Predicting customer churn, segmenting customers, or deciding whether a customer will respond to a marketing campaign.\n",
        "\n",
        "    Finance: Assessing credit risk, loan approval, and fraud detection.\n",
        "\n",
        "    Healthcare: Diagnosing diseases, recommending treatments, and predicting patient outcomes.\n",
        "\n",
        "    Engineering & Manufacturing: Detecting faults, quality control, and process optimization.\n",
        "\n",
        "    Advantages: Decision trees are easy to interpret and visualize, require little data preprocessing, and can handle both categorical and numerical data. They also capture non-linear relationships between features.\n",
        "\n",
        "    Limitations: They are prone to overfitting, especially with deep trees, and can be unstable, as small changes in data may produce a very different tree. Decision trees also tend to be less accurate compared to ensemble methods like Random Forests or Gradient Boosted Trees.\n"
      ],
      "metadata": {
        "id": "c1jHHNlAy3ji"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Q6) Write a Python program to:\n",
        "#● Load the Iris Dataset\n",
        "#● Train a Decision Tree Classifier using the Gini criterion\n",
        "#● Print the model’s accuracy and feature importances\n",
        "\n",
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data   # Features\n",
        "y = iris.target # Labels\n",
        "\n",
        "# Split dataset into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Decision Tree Classifier using Gini criterion\n",
        "clf = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Print accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Model Accuracy:\", accuracy)\n",
        "\n",
        "# Print feature importances\n",
        "print(\"Feature Importances:\")\n",
        "for feature, importance in zip(iris.feature_names, clf.feature_importances_):\n",
        "    print(f\"{feature}: {importance:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8EoqIXL8z4JQ",
        "outputId": "1e4c12fd-e628-40a8-e191-e8a1c89a5702"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 1.0\n",
            "Feature Importances:\n",
            "sepal length (cm): 0.0000\n",
            "sepal width (cm): 0.0167\n",
            "petal length (cm): 0.9061\n",
            "petal width (cm): 0.0772\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Q7) Write a Python program to:\n",
        "#● Load the Iris Dataset\n",
        "#● Train a Decision Tree Classifier with max_depth=3 and compare its accuracy to a fully-grown tree.\n",
        "\n",
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split dataset into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train a Decision Tree with max_depth=3\n",
        "tree_limited = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
        "tree_limited.fit(X_train, y_train)\n",
        "y_pred_limited = tree_limited.predict(X_test)\n",
        "accuracy_limited = accuracy_score(y_test, y_pred_limited)\n",
        "\n",
        "# Train a fully-grown Decision Tree\n",
        "tree_full = DecisionTreeClassifier(random_state=42)\n",
        "tree_full.fit(X_train, y_train)\n",
        "y_pred_full = tree_full.predict(X_test)\n",
        "accuracy_full = accuracy_score(y_test, y_pred_full)\n",
        "\n",
        "# Print accuracy comparison\n",
        "print(f\"Accuracy of tree with max_depth=3: {accuracy_limited:.4f}\")\n",
        "print(f\"Accuracy of fully-grown tree: {accuracy_full:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kgt0Admpz9sU",
        "outputId": "f3121d51-df51-4073-8c13-4805453ae760"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of tree with max_depth=3: 1.0000\n",
            "Accuracy of fully-grown tree: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Q8) Write a Python program to:\n",
        "#● Load the California Housing dataset from sklearn\n",
        "#● Train a Decision Tree Regressor\n",
        "#● Print the Mean Squared Error (MSE) and feature importances\n",
        "\n",
        "# Import necessary libraries\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load California Housing dataset\n",
        "california = fetch_california_housing()\n",
        "X = california.data\n",
        "y = california.target\n",
        "\n",
        "# Split dataset into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Decision Tree Regressor\n",
        "regressor = DecisionTreeRegressor(random_state=42)\n",
        "regressor.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = regressor.predict(X_test)\n",
        "\n",
        "# Calculate Mean Squared Error\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(f\"Mean Squared Error (MSE): {mse:.4f}\")\n",
        "\n",
        "# Print feature importances\n",
        "print(\"Feature Importances:\")\n",
        "for feature, importance in zip(california.feature_names, regressor.feature_importances_):\n",
        "    print(f\"{feature}: {importance:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rJtZyPri0HL5",
        "outputId": "984d61ae-067f-4e3b-9d14-ac6aa6933360"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error (MSE): 0.4952\n",
            "Feature Importances:\n",
            "MedInc: 0.5285\n",
            "HouseAge: 0.0519\n",
            "AveRooms: 0.0530\n",
            "AveBedrms: 0.0287\n",
            "Population: 0.0305\n",
            "AveOccup: 0.1308\n",
            "Latitude: 0.0937\n",
            "Longitude: 0.0829\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Q9) Write a Python program to:\n",
        "#● Load the Iris Dataset\n",
        "#● Tune the Decision Tree’s max_depth and min_samples_split using GridSearchCV\n",
        "#● Print the best parameters and the resulting model accuracy\n",
        "\n",
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split dataset into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define parameter grid for tuning\n",
        "param_grid = {\n",
        "    'max_depth': [2, 3, 4, 5, None],\n",
        "    'min_samples_split': [2, 5, 10, 15]\n",
        "}\n",
        "\n",
        "# Initialize Decision Tree Classifier\n",
        "dt = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "# Use GridSearchCV for hyperparameter tuning\n",
        "grid_search = GridSearchCV(estimator=dt, param_grid=param_grid, cv=5, scoring='accuracy')\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best parameters\n",
        "best_params = grid_search.best_params_\n",
        "print(\"Best Parameters:\", best_params)\n",
        "\n",
        "# Evaluate the best model on test data\n",
        "best_model = grid_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy with best parameters: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G9aCnk3h0LtU",
        "outputId": "f6b57209-094c-4682-9628-d6ff50d083d5"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'max_depth': 4, 'min_samples_split': 2}\n",
            "Model Accuracy with best parameters: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q10) Imagine you’re working as a data scientist for a healthcare company that\n",
        "wants to predict whether a patient has a certain disease. You have a large dataset with mixed data types and some missing values.\n",
        "Explain the step-by-step process you would follow to:\n",
        "● Handle the missing values\n",
        "● Encode the categorical features\n",
        "● Train a Decision Tree model\n",
        "● Tune its hyperparameters\n",
        "● Evaluate its performance\n",
        "And describe what business value this model could provide in the real-world\n",
        "setting.\n",
        "\n",
        "Ans10)\n",
        "\n",
        "    In a healthcare setting, predicting whether a patient has a certain disease requires careful handling of the dataset, especially when it contains mixed data types and missing values. The first step is to handle missing values, which can be done by imputing numerical features with measures like the mean or median, and categorical features with the mode, or by using more advanced methods like K-Nearest Neighbors imputation. Next, categorical features need to be converted into numerical form so that a Decision Tree can process them; this can be done using one-hot encoding or ordinal encoding depending on the nature of the categories. Once the data is cleaned and encoded, a Decision Tree classifier can be trained on the dataset, as it can handle both numerical and categorical variables and provides interpretable results. To improve the model’s performance, hyperparameter tuning can be performed, adjusting parameters such as max_depth, min_samples_split, and min_samples_leaf using techniques like GridSearchCV or RandomizedSearchCV. After training, the model should be evaluated using metrics like accuracy, precision, recall, F1-score, and the confusion matrix, especially because disease prediction often involves imbalanced classes.\n",
        "\n",
        "    The business value of such a model in a healthcare setting is significant: it can help clinicians identify high-risk patients early, prioritize testing and treatments, optimize resource allocation, and reduce healthcare costs. By providing data-driven insights, the model supports proactive decision-making, improves patient outcomes, and enhances operational efficiency in hospitals or clinics."
      ],
      "metadata": {
        "id": "ThB_LiOP0WQb"
      }
    }
  ]
}